{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f7955d-5b4a-4289-b661-abbc4bbe7ae4",
   "metadata": {},
   "source": [
    "## 5.3 트리의 앙상블\n",
    "\n",
    "목적: 앙상블 학습이 무엇인지 이해하고 다양한 앙상블 학습 알고리즘을 실습을 통해 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0733589-a11a-46b0-8b15-77a8b3fcf8a7",
   "metadata": {},
   "source": [
    "### 정형 데이터와 비정형 데이터\n",
    "\n",
    "- 정형 데이터(structured data): 어떤 구조로 되어 있는 자료로 엑셀이나 csv, 데이터베이스에 저장하기 쉬움\n",
    "- 비정형 데이터(unstructured data): 책의 글, 디지털카메라로 찍은 사진, 핸드폰으로 듣는 디지털 음악 등이 이에 속함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253707a-7a5a-4eb5-83c9-f6d5775a3aa6",
   "metadata": {},
   "source": [
    "-> 정현 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘이 <strong>앙상블 학습(ensemble learning)</strong> 이라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad230983-c6a6-49f6-9ba0-dec0fca6a5ae",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트(Random Forest)\n",
    "\n",
    "랜덤 포레스트는 랜덤하게 샘플을 추출하여 훈련 데이터를 만드는데, 이때 한 샘플이 중복되어 추출될 수도 있다.\n",
    "\n",
    "예를 들어, 1,000개 샘플이 들어 있는 가방에서 100개의 샘플을 뽑는다면 먼저 1개를 뽑고, 뽑았던 1개를 다시 가방에 넣는 식이다.\n",
    "\n",
    "이렇게 만들어지 데이터 샘플을 <strong>부트스트랩 샘플(bootstrap sample)</strong> 이라고 한다.\n",
    "\n",
    "다시 말해, 부트스트랩은 데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80514ad1-fe92-4965-aa9e-6ae1de189c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인 분류하는 문제에 적용\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine   = pd.read_csv(\"https://bit.ly/wine_csv_data\")\n",
    "data   = wine[[\"alcohol\", \"sugar\", \"pH\"]].to_numpy()\n",
    "target = wine[\"class\"].to_numpy()\n",
    "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6f9fa0-573c-4550-a70d-6da4db2b5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cross_validate()를 사용하여 교차 검증 수행\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf     = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ecd1cf-a9ee-4ed1-86e1-9de57d85a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973541965122431 0.8905151032797809\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores[\"train_score\"]), np.mean(scores[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7801bfa-b9a8-492c-a8bb-fabb5da07b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23167441 0.50039841 0.26792718]\n"
     ]
    }
   ],
   "source": [
    "rf.fit(train_input, train_target)\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e7c11-3947-4c9c-8243-2b3ec5adf377",
   "metadata": {},
   "source": [
    "RandomForesetClassifier에는 자체적으로 모델을 평가할 수 있는데, \n",
    "\n",
    "평가할 때 부트스트랩 샘플에 포함되지 않고 남은 샘플, 즉 OOB(out of Bag) 샘플을 가지고 평가할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7915866b-e0c3-4604-b26b-88fd8b62f67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8934000384837406\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
    "rf.fit(train_input, train_target)\n",
    "print(rf.oob_score_) # 이를 사용하면 교차 검증 대신 사용 가능 -> 훈련 세트에 더 많은 샘플을 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77696cb2-0af1-4f58-add8-5fab2455ec2b",
   "metadata": {},
   "source": [
    "### 엑스트라 트리(Extra tree)\n",
    "\n",
    "엑스트라 트리는 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는 데 사용한다.\n",
    "\n",
    "랜덤포레스트와 엑스트라 트리의 차이점은 부트스트랩을 사용하지 않는다는 점이다. 즉 각 결정 트리를 만들 때 전체 훈련 세트를 사용한다.\n",
    "\n",
    "대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지겠지만 많은 트리를 앙상블 하기 때문에 과대적합을 막고 점증 세트의 점수를 높이는 효과가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b4a49c9-5c36-4272-8fea-fc1a1992bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et     = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff71d3f5-e4b1-48dc-aaaf-b7b53883e0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974503966084433 0.8887848893166506\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores[\"train_score\"]), np.mean(scores[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975925e9-cdf3-4d2c-bc83-ceb7028a96b3",
   "metadata": {},
   "source": [
    "보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리를 훈련해야 한다.\n",
    "\n",
    "하지만 랜덤하게 노드를 분할하기 때문에 빠른 계산 속도가 엑스트라 트리의 장점이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e5d5625-093b-42dc-82b4-503310f1a990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20183568 0.52242907 0.27573525]\n"
     ]
    }
   ],
   "source": [
    "# 특성 중요도\n",
    "# 알코올 도수, 당도, pH 순서\n",
    "\n",
    "et.fit(train_input, train_target)\n",
    "print(et.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2949c21-7d84-4da1-8254-4e584871d15f",
   "metadata": {},
   "source": [
    "### 그레이디언트 부스팅(Gradient Boosting)\n",
    "\n",
    "그레디언트 부스틍은 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다.\n",
    "\n",
    "사이킷런에서는 기본적으로 깊이가 3인 결정 트리를 100개 사용한다.\n",
    "\n",
    "깊이가 얕은 결정 트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f89d8902-81cd-41d0-b323-5fedaccfd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb     = GradientBoostingClassifier(random_state=42)\n",
    "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b594943-1a5f-4267-af43-78ce7be34091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8881086892152563 0.8720430147331015\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores[\"train_score\"]), np.mean(scores[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764ea7a-fa8d-44cd-89c3-43a2f9bc3d05",
   "metadata": {},
   "source": [
    "위 결과는 과대적합이 되지 않았다. 그레이디언트 부스팅은 결정 트리의 개수를 늘려도 과대적합에 매우 강하다.\n",
    "\n",
    "학습률을 증가시키고 트리의 개수를 늘리면 조금 더 성능이 향상될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2a83ca4-e692-4471-a903-a873422ece6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GradientBoostingClassifier.__init__() got an unexpected keyword argument 'n_extimators'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gb     \u001b[38;5;241m=\u001b[39m \u001b[43mGradientBoostingClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_extimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_validate(gb, train_input, train_target, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: GradientBoostingClassifier.__init__() got an unexpected keyword argument 'n_extimators'"
     ]
    }
   ],
   "source": [
    "gb     = GradientBoostingClassifier(n_extimators=500, learning_rate=0.2, random_state=42)\n",
    "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe1b724-0865-45e3-bb24-779a6cb2ee00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
